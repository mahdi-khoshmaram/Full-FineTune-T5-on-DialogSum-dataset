<<<<<<< HEAD
# Full Fine-Tuning of a Pretrained T5 Model on the DialogSum Dataset  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1P7W3UsHSUDbFJgK0Mbd-OazySVa-Q7T8?usp=sharing)  

### Created by: [Mahdi Khoshmaram](https://github.com/mahdi-khoshmaram) ðŸ¤—  

## Overview  
This repository contains a Jupyter Notebook for fine-tuning the **T5** model on the [DialogSum dataset](https://huggingface.co/datasets/knkarthick/dialogsum). The notebook demonstrates two fine-tuning approaches:  
=======
# Full Fine-Tuning of a Pretrained T5 model on the DialogSum Dataset  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1P7W3UsHSUDbFJgK0Mbd-OazySVa-Q7T8?usp=sharing)  


## Overview  
This repository contains a Jupyter Notebook for full fine-tuning the **T5** Large Language Model on the [DialogSum dataset](https://huggingface.co/datasets/knkarthick/dialogsum). The notebook demonstrates two fine-tuning approaches:  
>>>>>>> 6c61c503a732c8061980c2e2a03e1f2d58927681

1. Fine-tuning with the ðŸ¤— **Transformers Trainer**  
2. Fine-tuning using **native PyTorch**  

## Getting Started  
Click the button below to open the notebook in Google Colab and start experimenting:  

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1P7W3UsHSUDbFJgK0Mbd-OazySVa-Q7T8?usp=sharing)  

<<<<<<< HEAD
## Dependencies  
To run the notebook locally, install the required libraries:  

```bash
pip install transformers datasets torch
=======
## Cloning the Repository  
To clone this repository to your local machine, run the following command:  

```bash
git clone [https://github.com/mahdi-khoshmaram/your-repo-name.git](https://github.com/mahdi-khoshmaram/Full-FineTuning-T5-on-DialogSum-dataset.git)
```

## ðŸŒŸ Support & Star the Repo!  

If this project helps you or you just love fine-tuning models as much as I do, consider giving it a â­!    
>>>>>>> 6c61c503a732c8061980c2e2a03e1f2d58927681
