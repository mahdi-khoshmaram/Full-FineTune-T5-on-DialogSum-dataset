{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WbTTG7tbC1mU",
        "iNBAM7HSXSs-",
        "liPT1hdPD-7d",
        "qM4s61P9rJmI",
        "hXFeW4aC9oGj",
        "KApS1AWMg9Iu",
        "qnvT-q-Ib4KH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Full Fine-Tuning of a pretrained T5 model on the DialogSum dataset**\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1P7W3UsHSUDbFJgK0Mbd-OazySVa-Q7T8?usp=sharing)\n",
        "\n",
        "#### **This Notebook is created by: [mahdi khoshmaram](https://github.com/mahdi-khoshmaram)** 🤗\n",
        "\n",
        "\n",
        "**In this notebook**, I fine-tune the **`T5`** model on the [DialogSum dataset](https://huggingface.co/datasets/knkarthick/dialogsum) using **two** approaches:\n",
        "1. Fine-tuning with the 🤗 **`Transformers` Trainer**.\n",
        "2. Fine-tuning using **native PyTorch.**"
      ],
      "metadata": {
        "id": "R4xP74JZBVjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. [Set Device](#scrollTo=iNBAM7HSXSs-&line=1&uniqifier=1)\n",
        "2. [Loading Dataset](#scrollTo=iNBAM7HSXSs-&line=1&uniqifier=1)\n",
        "3. [Set-Up Model](#scrollTo=op8CLNJRaY_B&line=1&uniqifier=1)\n",
        "4. [Make dataset ready for training](#scrollTo=liPT1hdPD-7d)\n",
        "5. [Full Fine-tuning with the 🤗 Transformers Trainer](#scrollTo=qM4s61P9rJmI&line=1&uniqifier=1)\n",
        "6. [Full Fine-tuning using native PyTorch](#scrollTo=hXFeW4aC9oGj&line=1&uniqifier=1)\n",
        "7. [Evaluating the Original and Fine-Tuned Models Using ROUGE](#scrollTo=KApS1AWMg9Iu)"
      ],
      "metadata": {
        "id": "z--RVKIEAhgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Device**\n",
        "[Back to Top](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "WbTTG7tbC1mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.device` is a PyTorch object that specifies the device (CPU or GPU) on which tensors are allocated. It helps manage computations efficiently across different hardware."
      ],
      "metadata": {
        "id": "jggmWxGRyjhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print(f\"Memory: {round((torch.cuda.get_device_properties(device).total_memory)/(1024)**3,2)}GB\")"
      ],
      "metadata": {
        "id": "vWCjaAzFpcnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dataset**\n",
        "\n",
        "[[Back to Top]](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "iNBAM7HSXSs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install datasets\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "4KXuFZlLRrLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "hf_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(hf_dataset_name, spilit=None)"
      ],
      "metadata": {
        "id": "V1lpPVPzUh30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set-Up Model**\n",
        "\n",
        "[[Back to Top]](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)\n"
      ],
      "metadata": {
        "id": "op8CLNJRaY_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoModelForSeq2SeqLM` is a class from the Hugging Face Transformers library. It is used to automatically load a sequence-to-sequence (Seq2Seq) model based on the model checkpoint you specify.\n",
        "\n",
        "---\n",
        "Seq2Seq models are commonly used for tasks like text translation, summarization, and text generation.\n",
        "\n",
        "---\n",
        "**GenerationConfig:**\n",
        "\n",
        "``max_new_tokens=200``\n",
        "* This sets the maximum number of new tokens the model can generate in response to a prompt.\n",
        "\n",
        "\n",
        "``do_sample=True``\n",
        "* This enables sampling-based generation instead of deterministic generation.\n",
        "* When ``do_sample=False``, the model chooses the most likely token at each step (greedy decoding or beam search).\n",
        "* When ``do_sample=True``, the model introduces randomness, making outputs more diverse.\n",
        "* **When ``do_sample=False``, the ``temperature`` parameter has no effect.**\n",
        "\n",
        "``temperature=1``\n",
        "* This controls the randomness of token selection during sampling.\n",
        "* A higher temperature ``(>1)`` makes the model more random and creative.\n",
        "* A lower temperature ``(<1)`` makes it more deterministic and focused.\n",
        "* ``Temperature=1`` means default randomness—balancing coherence and diversity.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What is ``torch_dtype=\"auto\"``**?\n",
        "\n",
        "\n",
        "* By default, when we load a model with:\n",
        "```python\n",
        "    AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "```\n",
        "The model loads its weights in ```torch.float32``` (full precision), even if the original model was trained or stored in a lower precision format like ```torch.float16```.\n",
        "\n",
        "**Using ```torch_dtype=\"auto\"```**\n",
        "Instead of manually specifying a precision (e.g., ```torch.float16``` or ```torch.bfloat16```), we can automatically load the model in the optimal data type by setting:\n",
        "```python\n",
        "AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=\"auto\")\n",
        "```\n",
        "* This tells PyTorch to check the model's ``config.json`` file, which defines the precision in which the weights were saved.\n",
        "* If the model was originally trained and stored in ``torch.float16`` or ``torch.bfloat16``, it will load in that format, saving memory and improving speed.\n",
        "---"
      ],
      "metadata": {
        "id": "gMvJ5DHBvm_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig"
      ],
      "metadata": {
        "id": "bFmG7Dj6K-qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_name = 'google/flan-t5-base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=\"auto\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "generation_config = GenerationConfig(max_new_tokens=200, do_sample=True, temperature=1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iIej9M-CXYD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make dataset ready for training**\n",
        "\n",
        "[[Back to Top]](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "liPT1hdPD-7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* **Map** method in dataset class: [ChatGPT response](https://chatgpt.com/share/67b9efea-5534-8004-bcf1-8562576077db)\n",
        "---"
      ],
      "metadata": {
        "id": "V1sfATH1UhXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(batch):\n",
        "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
        "    end_prompt = \"\\n\\nSummary: \"\n",
        "    prompts = [start_prompt + dialogue + end_prompt for dialogue in batch['dialogue']]\n",
        "    batch['input_ids'] = tokenizer(prompts, padding='max_length', truncation=True, return_tensors='pt').input_ids\n",
        "    batch['labels'] = tokenizer(batch['summary'], padding='max_length', truncation=True, return_tensors='pt').input_ids\n",
        "    return batch\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'dialogue', 'summary', 'topic'])"
      ],
      "metadata": {
        "id": "g_o9Gbs-D9k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "* About **Lambda** function: https://chatgpt.com/share/67b9fd12-ac54-8004-b30c-6bee1d584f2e\n",
        "\n",
        "\n",
        "* About **filter** method arguments of huggingface **dataset** class: https://chatgpt.com/share/67b9ff1e-85f8-8004-970a-d53772de81ac\n",
        "---"
      ],
      "metadata": {
        "id": "sOaWyWtlhPIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.filter(lambda example, index: index % 10 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "IXU3EVTuyhDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full Fine-tuning with the 🤗 Transformers [Trainer](https://huggingface.co/docs/transformers/en/training#train-with-pytorch-trainer) class**\n",
        "\n",
        "[[Back to Top]](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)\n"
      ],
      "metadata": {
        "id": "qM4s61P9rJmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**TrainingArguments**\n",
        "\n",
        "``TrainingArguments`` is a class in the 🤗 `Transformers` library that is used to configure the training process of a model. It provides various options for fine-tuning and training models using the Trainer API.\n",
        "\n",
        "* ``TrainingArguments`` class is often used with the ``Trainer`` class for model training in Hugging Face Transformers.\n",
        "\n",
        "* ``TrainingArguments`` is the subset of the arguments we use in our example scripts **which relate to the training loop itself.**\n",
        "\n",
        "---\n",
        "**Training hyperparameters**\n",
        "\n",
        "`output_dir`\n",
        "* Specifies the directory where model checkpoints and logs will be saved.\n",
        "\n",
        "`learning_rate`\n",
        "* Sets the learning rate for the optimizer.\n",
        "\n",
        "`num_train_epochs`\n",
        "* Defines the total number of training epochs\n",
        "\n",
        "`weight_decay`\n",
        "* Regularization technique to prevent overfitting by adding a penalty to large weights.\n",
        "\n",
        "`logging_steps`\n",
        "* Determines how often training logs (such as loss values) are recorded.\n",
        "\n",
        "`per_device_train_batch_size`\n",
        "* Number of training samples per batch per device\n",
        "\n",
        "`per_device_eval_batch_size`\n",
        "* Number of evaluation (validation) samples per batch per device\n",
        "\n",
        "`eval_strategy`\n",
        "* Defines how often evaluation is performed.\n",
        "\n",
        "`report_to`\n",
        "* Specifies where to log training metrics (e.g., `wandb`, `tensorboard`).\n",
        "* \"none\" disables logging to external tracking tools.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "j9MvwWWiL5jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from time import strftime"
      ],
      "metadata": {
        "id": "gXMeAUh98sYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_training_args = TrainingArguments(\n",
        "    output_dir = f\"./T5-FF-TransformersTrainer-{strftime('%H:%M:%S')}\",\n",
        "    learning_rate = 1e-5,\n",
        "    num_train_epochs = 10,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    eval_strategy = \"epoch\",\n",
        "    report_to = \"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "p3BvqfoFSsDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Now, fine-tune the model**\n",
        "\n",
        "Create a Trainer object with your **model**, **training arguments**, **training and test datasets**, and **evaluation function**:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "w7heweupfwhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer"
      ],
      "metadata": {
        "id": "qA1MHexof0sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = HF_training_args,\n",
        "    train_dataset = tokenized_dataset[\"train\"],\n",
        "    eval_dataset = tokenized_dataset[\"validation\"]\n",
        ")"
      ],
      "metadata": {
        "id": "_I8ZjpZdf4hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "yPPFNP-Hh4jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full Fine-tuning using native PyTorch**\n",
        "\n",
        "[[Back to Top]](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "hXFeW4aC9oGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In **Hugging Face Datasets**, `set_format(\"torch\")` is used to **convert dataset elements into PyTorch tensors.**\n",
        "\n",
        "This is useful when training a model with PyTorch, as it ensures that the data is in the correct format.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "E6txeW5j_PwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "IAX1O22c9m_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**DataLoader**\n",
        "\n",
        "The `Dataset` retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
        "\n",
        "**`DataLoader`** is an iterable that abstracts this complexity for us in an easy API. [link](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders)\n",
        "\n",
        "**DataLoader Parameters:**\n",
        "* `dataset`: The dataset to load data from.\n",
        "* `batch_size`: Number of samples per batch (default is `1`).\n",
        "* `shuffle`: Whether to shuffle the data at every epoch (`True`/`False`).\n",
        "* `num_workers`: Number of CPU processes used for data loading (`0` means no parallel loading).\n",
        "* `pin_memory`: If `True`, speeds up GPU transfer by using pinned (page-locked) memory.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2CnzwDYIA5GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset=tokenized_dataset['train'], shuffle=True, batch_size=4)\n",
        "eval_dataloader = DataLoader(dataset=tokenized_dataset[\"validation\"], batch_size=4)"
      ],
      "metadata": {
        "id": "ADj4O7-9BUPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Optimizer**\n",
        "\n",
        "Create an optimizer and learning rate scheduler to fine-tune the model. Let’s use the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer from PyTorch:\n",
        "\n",
        "* `torch.optim` is a package implementing various optimization algorithms.\n",
        "* To construct an Optimizer I have to give it an iterable containing the **parameters** to optimize. Then, I can specify **optimizer-specific options** such as the learning rate, weight decay, etc.\n",
        "\n",
        "* In PyTorch, **`model.parameters()`** is a method that returns an iterator over all the learnable parameters (i.e., weights and biases) of a neural network model.\n",
        "\n",
        "---\n",
        "**Learning rate scheduler - `get_scheduler`**\n",
        "\n",
        "When fine-tuning a pre-trained model (like BERT, GPT, or ViT), gradually decreasing the learning rate (LR) over time helps improve stability and ensures the model adapts well to the new task without forgetting pre-trained knowledge. Here’s why:\n",
        "1. Prevents Catastrophic Forgetting\n",
        "2. Helps Convergence & Avoids Overshooting\n",
        "3. Improves Generalization & Reduces Overfitting\n",
        "\n",
        "**`get_scheduler`** is a utility from `transformers` helps adjust the learning rate dynamically.\n",
        "**`get_scheduler`** parameters:\n",
        "\n",
        "* `name`: Specifies the type of scheduler to use. Each scheduler adjusts the learning rate differently during training. Available options include: **linear**, **cosine**, **cosine_with_restarts**, **polynomial**, **constant**, **constant_with_warmup**, **inverse_sqrt**, **reduce_lr_on_plateau**, **cosine_with_min_lr**, **warmup_stable_decay**.\n",
        "<br>\n",
        "\n",
        "* `optimizer`:The optimizer that will be used during training.\n",
        "* `num_warmup_steps`: Number of steps to linearly increase the learning rate from 0 to the initial value set in the optimizer.\n",
        "* `num_training_steps`: Total number of training steps. This is typically calculated as the **number of epochs multiplied by the number of batches per epoch**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4sMcH1w8DxC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Optimizer\n",
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "3LD38AlED-On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set lr scheduler\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "ZdAoGW8YIEOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Lastly**, specify `device` to use a GPU if you have access to one. Otherwise, training on a CPU may take several hours instead of a couple of minutes.\n",
        "\n",
        "* `get_backend()` is used for automatically detecting the best available computing device (e.g., GPU or CPU).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XqGdCwdKQd9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from accelerate.test_utils.testing import get_backend\n",
        "\n",
        "device, _, _ = get_backend()\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t7J0H2W3QdZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Training Loop**\n",
        "\n",
        "`model.train()`\n",
        "* This puts the model into \"training mode,\" enabling behaviors such as dropout (if applicable) and tracking gradients.\n",
        "\n",
        "`batch = {k: torch.tensor(v).to(device) for k, v in batch.items()}`\n",
        "* Preparing Data for the Model: Converts each value in batch to a PyTorch tensor and moves it to the specified device (GPU or CPU)\n",
        "\n",
        "`output = model(**batch)`\n",
        "* Forward Pass : Feeds the batch into the model, producing an output object.\n",
        "* The output typically contains predictions and a loss value (if the model is set up for training).\n",
        "\n",
        "`loss = output.loss`\n",
        "* Extracts the loss from the model's output.\n",
        "\n",
        "`loss.backward()`\n",
        "* Computes the gradients of the loss with respect to the model parameters (backpropagation).\n",
        "\n",
        "`optimizer.step()`\n",
        "* Updates model parameters using the gradients computed in `loss.backward()`\n",
        "\n",
        "`lr_scheduler.step()`\n",
        "* Adjusts the learning rate according to the learning rate scheduler (if used).\n",
        "\n",
        "`optimizer.zero_grad()`\n",
        "* Clears previously accumulated gradients before the next iteration. This prevents gradient accumulation across batches.\n",
        "* In PyTorch, gradients are accumulated by default. This means that after calling `.backward()`, the gradients from the current backward pass add up to the gradients from previous batches.\n",
        "* `optimizer.zero_grad()` resets (clears) all gradients of the model parameters before the next iteration, preventing unintended accumulation.\n",
        "\n",
        "`progress_bar.update(1)`\n",
        "* Moves the progress bar forward by one step to indicate progress.\n",
        "\n",
        "`model.save_pretrained(output_dir)`\n",
        "* Saves the trained model to the specified output_dir, allowing it to be reloaded later.\n",
        "\n",
        "---\n",
        "\n",
        "**Why Are Gradients Accumulated?**\n",
        "\n",
        "In PyTorch, when you call `loss.backward()`, the gradients are not automatically replaced. Instead, they are added to the existing gradients.\n",
        "\n",
        "This behavior is useful in cases like gradient accumulation, where you intentionally sum gradients over multiple batches before updating the weights. However, in a standard training loop, this would cause issues if gradients were not reset before each step.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Hbdi7kM0hFpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from time import strftime\n",
        "output_dir = f\"./T5-FF-NativePytorch-{strftime('%H:%M:%S')}\"\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: torch.tensor(v).to(device) for k, v in batch.items()}\n",
        "        output = model(**batch)\n",
        "        loss = output.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "08nIifMJg1eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating the Original and Fine-Tuned Models Using ROUGE**\n",
        "\n",
        "[[Back to Top]](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "KApS1AWMg9Iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Evaluating the Original Model vs. Fine-Tuned Models**\n",
        "\n",
        "In this section, to determine whether the fine-tuned model's performance has improved, I created the `compute_rouge` function that takes a `model_name` as input and returns the **ROUGE** metrics for evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "**The default number of examples for calculating ROUGE is set to 10. This can be adjusted using the `num_examples` parameter.**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Weiq2R5qWXxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install evaluate\n",
        "%pip install rouge_score"
      ],
      "metadata": {
        "id": "Yb3KpC4jbwef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def compute_rouge(model_name, tokenizer=tokenizer, dataset=dataset ,num_examples = 10):\n",
        "    print(f\"Model:____{model_name}____\\n\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=\"auto\").to(device)\n",
        "    dialogues = dataset['test'][0:num_examples]['dialogue']\n",
        "    prediction_list = []\n",
        "    for index, dialogue in enumerate(dialogues):\n",
        "        prompt = f\"\"\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary: \"\"\"\n",
        "        input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
        "        model_output = model.generate(input_ids, generation_config=generation_config)\n",
        "        model_text_output = tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
        "        prediction_list.append(model_text_output)\n",
        "\n",
        "    rouge = evaluate.load('rouge')\n",
        "    rouge_score = rouge.compute(\n",
        "        predictions = prediction_list,\n",
        "        references = dataset['test'][0:num_examples]['summary'],\n",
        "        use_aggregator = True,\n",
        "        use_stemmer = True)\n",
        "    return rouge_score"
      ],
      "metadata": {
        "id": "RiOhwtFYg8DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting Model names\n",
        "original_model = model_name\n",
        "# FF_model = \"./T5-FF-NativePytorch-\"\n",
        "FF_model = \"./T5-FF-TransformersTrainer-22:22:13/checkpoint-500\"\n",
        "\n",
        "# Compute ROUGE\n",
        "print(compute_rouge(model_name=original_model, num_examples=20), end=\"\\n\\n\")\n",
        "print(compute_rouge(model_name=FF_model, num_examples=20))"
      ],
      "metadata": {
        "id": "uKFOn_ic0GJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **T5 vocabulary with indices:**\n",
        "\n",
        "[[Back to Top]](#scrollTo=z--RVKIEAhgE&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "qnvT-q-Ib4KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_index_token = {v:k for k,v in tokenizer.get_vocab().items()}"
      ],
      "metadata": {
        "id": "4RiCQNmyY6ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to get logits in T5"
      ],
      "metadata": {
        "id": "6noZZhMScDGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "input_text = \"translate English to French: Hello, how are you?\"\n",
        "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
        "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]], device=device)"
      ],
      "metadata": {
        "id": "KE8NQHW60DQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)"
      ],
      "metadata": {
        "id": "erPkXlIe0mcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = torch.softmax(output.logits, dim=-1)"
      ],
      "metadata": {
        "id": "44reW612XR04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_token = torch.argmax(probs, dim=-1).item()"
      ],
      "metadata": {
        "id": "c-OPGOvrXjnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_index_token.get(pred_token)"
      ],
      "metadata": {
        "id": "tldvTRqpX8Va"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}